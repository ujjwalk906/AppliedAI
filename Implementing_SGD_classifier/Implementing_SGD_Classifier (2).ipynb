{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementing SGD Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eiDWcM_MC3H"
      },
      "source": [
        "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfe2NTQtLq11"
      },
      "source": [
        "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5DSPCLxqT-"
      },
      "source": [
        "<font color='red'> Importing packages</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Et8BKIxnsp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import linear_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpSk3WQBx7TQ"
      },
      "source": [
        "<font color='red'>Creating custom dataset</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsMp0oWzx6dv"
      },
      "source": [
        "# please don't change random_state\n",
        "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
        "# make_classification is used to create custom dataset \n",
        "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8W2fg1cyGdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8b831d-84aa-42ae-dcd6-ce3df2921169"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 15), (50000,))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x99RWCgpqNHw"
      },
      "source": [
        "<font color='red'>Splitting data into train and test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kh4dBfVyJMP"
      },
      "source": [
        "#please don't change random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gONY1YiDq7jD"
      },
      "source": [
        "# Standardizing the data.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DR_YMBsyOci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0475b7-5f5c-4f97-a2c2-39b0b2ee5dc2"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((37500, 15), (37500,), (12500, 15), (12500,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW4OHswfqjHR"
      },
      "source": [
        "# <font color='red' size=5>SGD classifier</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HpvTwDHyQQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a7a5d8-1a50-4553-f317-45423df53ee6"
      },
      "source": [
        "# alpha : float\n",
        "# Constant that multiplies the regularization term. \n",
        "\n",
        "# eta0 : double\n",
        "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
        "\n",
        "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf\n",
        "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYaVyQ2lyXcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d34993f-c137-493e-cf80-2bc70a70fb09"
      },
      "source": [
        "clf.fit(X=X_train, y=y_train) # fitting our model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.70, NNZs: 15, Bias: -0.501317, T: 37500, Avg. loss: 0.552526\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1.04, NNZs: 15, Bias: -0.752393, T: 75000, Avg. loss: 0.448021\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1.26, NNZs: 15, Bias: -0.902742, T: 112500, Avg. loss: 0.415724\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.43, NNZs: 15, Bias: -1.003816, T: 150000, Avg. loss: 0.400895\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.55, NNZs: 15, Bias: -1.076296, T: 187500, Avg. loss: 0.392879\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.65, NNZs: 15, Bias: -1.131077, T: 225000, Avg. loss: 0.388094\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.73, NNZs: 15, Bias: -1.171791, T: 262500, Avg. loss: 0.385077\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.80, NNZs: 15, Bias: -1.203840, T: 300000, Avg. loss: 0.383074\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.86, NNZs: 15, Bias: -1.229563, T: 337500, Avg. loss: 0.381703\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.90, NNZs: 15, Bias: -1.251245, T: 375000, Avg. loss: 0.380763\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1.94, NNZs: 15, Bias: -1.269044, T: 412500, Avg. loss: 0.380084\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1.98, NNZs: 15, Bias: -1.282485, T: 450000, Avg. loss: 0.379607\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 2.01, NNZs: 15, Bias: -1.294386, T: 487500, Avg. loss: 0.379251\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 2.03, NNZs: 15, Bias: -1.305805, T: 525000, Avg. loss: 0.378992\n",
            "Total training time: 0.15 seconds.\n",
            "Convergence after 14 epochs took 0.15 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAfkVI6GyaRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab2de2cd-80a3-4b93-e0b7-8923071f9002"
      },
      "source": [
        "clf.coef_, clf.coef_.shape, clf.intercept_\n",
        "#clf.coef_ will return the weights\n",
        "#clf.coef_.shape will return the shape of weights\n",
        "#clf.intercept_ will return the intercept term"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.89007184,  0.63162363, -0.07594145,  0.63107107, -0.38434375,\n",
              "          0.93235243, -0.89573521, -0.07340522,  0.40591417,  0.4199991 ,\n",
              "          0.24722143,  0.05046199, -0.08877987,  0.54081652,  0.06643888]]),\n",
              " (1, 15),\n",
              " array([-1.30580538]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-CcGTKgsMrY"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1_8bdzitDlM"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "1.  We will be giving you some functions, please write code in that functions only.\n",
        "\n",
        "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU2Y3-FQuJ3z"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
        "\n",
        "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
        "\n",
        " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
        "- for each epoch:\n",
        "\n",
        "    - for each batch of data points in train: (keep batch size=1)\n",
        "\n",
        "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
        "\n",
        "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
        "\n",
        "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
        "\n",
        "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
        "\n",
        "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
        "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
        "\n",
        "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
        "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
        "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
        "        you can stop the training\n",
        "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_HgjgS_wKu"
      },
      "source": [
        "<font color='blue'>Initialize weights </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GecwYV9fsKZ9"
      },
      "source": [
        "def initialize_weights(dim):\n",
        "    ''' In this function, we will initialize our weights and bias'''\n",
        "    #initialize the weights to zeros array of (1,dim) dimensions\n",
        "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
        "    #initialize bias to zero\n",
        "\n",
        "    w = np.zeros_like((dim))\n",
        "    b = 0\n",
        "\n",
        "    return w,b"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7I6uWBRsKc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643b374d-7479-465a-ab61-a8278908d8a8"
      },
      "source": [
        "dim=X_train[0] \n",
        "w,b = initialize_weights(dim)\n",
        "print('w =',(w))\n",
        "print('b =',str(b))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "b = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MI5SAjP9ofN"
      },
      "source": [
        "<font color='cyan'>Grader function - 1 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv1llH429wG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e4ef31-65d7-47a6-afc0-f043bde805f7"
      },
      "source": [
        "dim=X_train[0] \n",
        "w,b = initialize_weights(dim)\n",
        "def grader_weights(w,b):\n",
        "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
        "  return True\n",
        "grader_weights(w,b)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN83oMWy_5rv"
      },
      "source": [
        "<font color='blue'>Compute sigmoid </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPv4NJuxABgs"
      },
      "source": [
        "$sigmoid(z)= 1/(1+exp(-z))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAfmQF47_Sd6"
      },
      "source": [
        "def sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    # compute sigmoid(z) and return\n",
        "    sig = 1/(1+np.exp(-z))\n",
        "    return sig"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YrGDwg3Ae4m"
      },
      "source": [
        "<font color='cyan'>Grader function - 2</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_JASp_NAfK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b818cc-6e90-41e2-a00d-a9f8269ffe51"
      },
      "source": [
        "def grader_sigmoid(z):\n",
        "  val=sigmoid(z)\n",
        "  assert(val==0.8807970779778823)\n",
        "  return True\n",
        "grader_sigmoid(2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS7JXbcrBOFF"
      },
      "source": [
        "<font color='blue'> Compute loss </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfEiS22zBVYy"
      },
      "source": [
        "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaFDgsp3sKi6"
      },
      "source": [
        "def logloss(y_true,y_pred):\n",
        "    '''In this function, we will compute log loss '''\n",
        "    n = len(y_true)\n",
        "    sum = 0\n",
        "    for i in range(len(y_true)):\n",
        "      sum = sum + (y_true[i]*np.log10(y_pred[i]) + (1-y_true[i])*np.log10(1-y_pred[i]))\n",
        "\n",
        "    loss = -1 * (1/n*sum)\n",
        "    return loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs1BTXVSClBt"
      },
      "source": [
        "<font color='cyan'>Grader function - 3 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzttjvBFCuQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e018b4-8f23-493f-83a2-dfead477b538"
      },
      "source": [
        "def grader_logloss(true,pred):\n",
        "  loss=logloss(true,pred)\n",
        "  assert(loss==0.07644900402910389)\n",
        "  return True\n",
        "true=[1,1,0,1,0]\n",
        "pred=[0.9,0.8,0.1,0.8,0.2]\n",
        "grader_logloss(true,pred)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQabIadLCBAB"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to  'w' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTMxiYKaCQgd"
      },
      "source": [
        "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVikyuFsKo5"
      },
      "source": [
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "    '''In this function, we will compute the gardient w.r.to w '''\n",
        "    w = np.array(w)\n",
        "    x = np.array(x)\n",
        "    sig_term = np.dot(w,x) + b\n",
        "    dw = x*(y - sigmoid(sig_term))-(alpha/N)*w\n",
        "    return dw"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFLNqL_GER9"
      },
      "source": [
        "<font color='cyan'>Grader function - 4 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI3xD8ctGEnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3bd0254-44b9-4c05-8df6-6e1165cd2e20"
      },
      "source": [
        "def grader_dw(x,y,w,b,alpha,N):\n",
        "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
        "  assert(np.sum(grad_dw)==2.613689585)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE8g84_GI62n"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to 'b' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHvTYZzZJJ_N"
      },
      "source": [
        "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nUf2ft4EZp8"
      },
      "source": [
        " def gradient_db(x,y,w,b):\n",
        "     '''In this function, we will compute gradient w.r.to b '''\n",
        "     w = np.array(w)\n",
        "     x = np.array(x)\n",
        "     sig_term = (np.dot(w,x) + b)\n",
        "     db = y - sigmoid(sig_term)\n",
        "     return (db)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbcBzufVG6qk"
      },
      "source": [
        "<font color='cyan'>Grader function - 5 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfFDKmscG5qZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451b6caf-cbed-4099-c3e4-c94eeb9e2be1"
      },
      "source": [
        "def grader_db(x,y,w,b):\n",
        "  grad_db=gradient_db(x,y,w,b)\n",
        "  assert(grad_db==-0.5)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_db(grad_x,grad_y,grad_w,grad_b)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tglnh4oxV8z-"
      },
      "source": [
        "    #Here eta0 is learning rate\n",
        "    #implement the code as follows\n",
        "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
        "    # for every epoch\n",
        "        # for every data point(X_train,y_train)\n",
        "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
        "           #compute gradient w.r.to b (call the gradient_db() function)\n",
        "           #update w, b\n",
        "        # predict the output of x_train[for all data points in X_train] using w,b\n",
        "        #compute the loss between predicted and actual values (call the loss function)\n",
        "        # store all the train loss values in a list\n",
        "        # predict the output of x_test[for all data points in X_test] using w,b\n",
        "        #compute the loss between predicted and actual values (call the loss function)\n",
        "        # store all the test loss values in a list\n",
        "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCK0jY_EOvyU"
      },
      "source": [
        "<font color='blue'> Implementing logistic regression</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmAdc5ejEZ25"
      },
      "source": [
        "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
        "    grad_w,grad_b=initialize_weights(X_train[0])\n",
        "    w = grad_w\n",
        "    b = grad_b\n",
        "    N = len(X_train)\n",
        "    training_losses = []\n",
        "    test_losses =[]\n",
        "    for epoch in range(epochs):\n",
        "      \n",
        "      for i in range(len(X_train)):\n",
        "        w = w + eta0*(gradient_dw(X_train[i],y_train[i],w,b,alpha,1))\n",
        "        b = b + eta0*(gradient_db(X_train[i],y_train[i],w,b))\n",
        "\n",
        "      predicted_outputs = []\n",
        "      for xis in X_train:\n",
        "        prediction = sigmoid(np.dot(w,xis)+b)\n",
        "        predicted_outputs.append(prediction)\n",
        "\n",
        "      training_loss = logloss(y_train,predicted_outputs)\n",
        "\n",
        "      training_losses.append(training_loss)\n",
        "\n",
        "      predicted_outputs = []\n",
        "      for xis in X_test:\n",
        "        prediction = sigmoid((np.dot(w,xis))+b)\n",
        "        predicted_outputs.append(prediction)\n",
        "\n",
        "      test_loss = logloss(y_test,predicted_outputs)\n",
        "      test_losses.append(test_loss)\n",
        "\n",
        "      # if epoch - 1 < 0:\n",
        "      #   continue\n",
        "      # elif training_losses[epoch] - training_losses[epoch - 1] <= 10**(-3):\n",
        "      #   break\n",
        "    \n",
        "    return w,b,training_losses,test_losses"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zi-24Ayl4cC",
        "outputId": "3313d92d-078e-4899-aae1-622c806c105f"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.39348337, -0.19771903, -0.15037836, -0.21528098, -1.28594363,\n",
              "       -0.66049132,  0.04140556, -0.22680269, -0.511055  , -0.42871073,\n",
              "        0.4210912 ,  0.22560347, -0.6624427 , -0.68888516,  0.56015427])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUquz7LFEZ6E"
      },
      "source": [
        "alpha=0.0001\n",
        "eta0=0.0001\n",
        "N=len(X_train)\n",
        "epochs=14\n",
        "w,b,training_loss,test_loss=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfZfvNji7EFv",
        "outputId": "0ec9591f-3194-4c31-c75b-0980751f37b8"
      },
      "source": [
        "training_loss"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.20730743828560452,\n",
              " 0.18557444165638973,\n",
              " 0.17661065321413408,\n",
              " 0.17202809704915442,\n",
              " 0.16939567529821759,\n",
              " 0.167769034933449,\n",
              " 0.16671310028613434,\n",
              " 0.16600314667745586,\n",
              " 0.16551323189540165,\n",
              " 0.16516836920015793,\n",
              " 0.16492179909159105,\n",
              " 0.16474329281661282,\n",
              " 0.1646127403050518,\n",
              " 0.16451645056188355]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4Zf_wPARlwY"
      },
      "source": [
        "<font color='red'>Goal of assignment</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3eF_VSPSH2z"
      },
      "source": [
        "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH_mHJdVjswD",
        "outputId": "6ed0a19a-a39b-42c7-8075-37dec458e690"
      },
      "source": [
        "#Difference between my Implementation and sklearn's implementation\n",
        "#The difference betwwen the weights are in range of 10^-3\n",
        "w-clf.coef_, b-clf.intercept_"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.00339402,  0.00658169,  0.00228187, -0.00088758,  0.00245189,\n",
              "          0.00104511,  0.0002263 ,  0.00226425,  0.00463724, -0.00542849,\n",
              "          0.00079289,  0.00241377,  0.00196633, -0.00202972,  0.00111738]]),\n",
              " array([0.003579]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "230YbSgNSUrQ"
      },
      "source": [
        "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
        "\n",
        "* epoch number on X-axis\n",
        "* loss on Y-axis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "xnN5-JXQiHjr",
        "outputId": "de8673e2-8e25-4687-f3a1-b3eec7545b7b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(range(epochs),training_loss,label=\"Training_loss\")\n",
        "plt.plot(range(epochs),test_loss,label=\"Test_loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc5a9654a90>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUN8puFoEZtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2749f78-9878-4cd2-9843-d38e5412758f"
      },
      "source": [
        "def pred(w,b, X):\n",
        "    N = len(X)\n",
        "    predict = []\n",
        "    for i in range(N):\n",
        "        z=np.dot(w,X[i])+b\n",
        "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
        "            predict.append(1)\n",
        "        else:\n",
        "            predict.append(0)\n",
        "    return np.array(predict)\n",
        "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
        "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9504533333333334\n",
            "0.9476\n"
          ]
        }
      ]
    }
  ]
}